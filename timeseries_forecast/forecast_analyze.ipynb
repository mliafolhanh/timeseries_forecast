{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Window\n",
    "from datalabframework.spark.utils import unidecode\n",
    "import pandas as pd\n",
    "import os\n",
    "from datalabframework.spark import dataframe\n",
    "import numpy as np\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import periodogram\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "import scipy.stats as scs\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
    "import math\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fact\n",
    "dlf = datalabframework.project.load()\n",
    "engine = dlf.engine()\n",
    "spark = engine.context()\n",
    "fact_transaction = engine.load('fact_table').select('sku_id', 'sku_name', 'transaction_date', 'quantity', 'doc_type', 'unit_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract quantity \n",
    "product_quantity_date = fact_transaction.where(F.expr('doc_type == \"PTX\"') | F.expr('doc_type == \"HDF\"'))\\\n",
    "                .where(F.expr('unit_price != 0'))\\\n",
    "                .groupby('sku_id', 'sku_name', 'transaction_date')\\\n",
    "                .agg(F.sum('quantity').alias('daily_total'), F.avg('unit_price').alias('daily_price'))\\\n",
    "                .orderBy('transaction_date')\n",
    "#extract_all_quantity\n",
    "product_quantity = fact_transaction.where(F.expr('doc_type == \"PTX\"') | F.expr('doc_type == \"HDF\"'))\\\n",
    "                                   .groupby('sku_id', 'sku_name')\\\n",
    "                                   .agg(F.sum('quantity').alias('total_quantity'), F.avg('unit_price').alias('avg_price'))\\\n",
    "                                   .orderBy(F.desc('total_quantity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quantity.show(20, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_product = product_quantity_date.where('sku_id != \"1206838\"')\\\n",
    "                                     .where('sku_id != \"1207652\"')\\\n",
    "                                     .groupby('transaction_date').agg(F.sum(F.col('daily_total')).alias('daily_total_product'))\n",
    "total_product.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an arbitrary product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ex_product = product_quantity_date.where('sku_id == 1204431').toPandas()\n",
    "one_ex_product['transaction_date'] = one_ex_product['transaction_date'].astype(np.datetime64)\n",
    "one_ex_product['daily_total'] = one_ex_product['daily_total'].astype(np.int64)\n",
    "one_ex_product['daily_price'] = one_ex_product['daily_price'].astype(np.float)/1000000\n",
    "start_date = one_ex_product['transaction_date'].min()\n",
    "end_date = one_ex_product['transaction_date'].max()\n",
    "one_ex_product = one_ex_product.set_index('transaction_date')\n",
    "full_date = pd.DataFrame({'date_ts':pd.date_range(start_date, end_date)}).set_index('date_ts')\n",
    "full_quantity_product = one_ex_product.join(full_date, how = 'outer').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_total_product = total_product.toPandas()\n",
    "pd_total_product['daily_total_product'] = pd_total_product['daily_total_product'].astype(np.int64)\n",
    "start_date = pd_total_product['transaction_date'].min()\n",
    "end_date = pd_total_product['transaction_date'].max()\n",
    "pd_total_product = pd_total_product.set_index('transaction_date')\n",
    "full_date = pd.DataFrame({'date_ts':pd.date_range(start_date, end_date)}).set_index('date_ts')\n",
    "full_quantity_total_product = pd_total_product.join(full_date, how = 'right').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quantity_total_product['week'] = full_quantity_total_product.index.to_series().dt.week\n",
    "full_quantity_total_product['year'] = full_quantity_total_product.index.to_series().dt.year\n",
    "full_quantity_total_product['month'] = full_quantity_total_product.index.to_series().dt.month\n",
    "full_quantity_total_product['dow'] = full_quantity_total_product.index.to_series().dt.dayofweek\n",
    "full_quantity_total_product['dom'] = full_quantity_total_product.index.to_series().dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_week= full_quantity_total_product.groupby('dow').agg({'daily_total_product': 'mean'})\n",
    "agg_week.columns = ['agg_total_week']\n",
    "agg_week.loc[4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_value_tet_holiday(row):\n",
    "    if row['daily_total_product'] == 0:\n",
    "        return agg_week.loc[row.loc['dow']].values[0]\n",
    "    return row['daily_total_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quantity_total_product['daily_total_product'] = full_quantity_total_product.apply(fix_value_tet_holiday, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quantity_total_product_dummies = pd.get_dummies(full_quantity_total_product, prefix = ['dow', 'moy', 'dom'], columns = ['dow', 'month', 'dom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_info = pd.read_csv('special_day.csv')\n",
    "holiday_info.columns = ['date', 'special_day_code', 'special_day_name']\n",
    "holiday_info = holiday_info.set_index('date')\n",
    "full_quantity_total_product_dummies_holi = full_quantity_total_product_dummies.join(holiday_info)\n",
    "full_quantity_total_product_dummies_holi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quantity_total_product_dummies_holi = pd.get_dummies(full_quantity_total_product_dummies_holi, prefix = 'holiday', \\\n",
    "                                                         columns = ['special_day_code'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quantity_total_product_dummies_holi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quantity_total_product_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test satationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(full_quantity_total_product['daily_total_product'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 6))\n",
    "plt.plot(full_quantity_total_product_dummies.index, full_quantity_total_product_dummies['daily_total_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(full_quantity_total_product['daily_total_product'].rolling(120).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize mean over time\n",
    "x = full_quantity_total_product['daily_total_product'].values\n",
    "y = [np.mean(x[:t]) for t in range(len(x))]\n",
    "plt.plot(y[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize variance over time\n",
    "x = full_quantity_total_product['daily_total_product'].values\n",
    "y = [np.std(x[:t]) for t in range(len(x))]\n",
    "plt.plot(y[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF\n",
    "smt.graphics.plot_acf(full_quantity_total_product['daily_total_product'], lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACF\n",
    "smt.graphics.plot_pacf(full_quantity_total_product['daily_total_product'], lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try diff 1\n",
    "x_diff1 = full_quantity_total_product['daily_total_product'].diff(1)\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.plot(x_diff1)\n",
    "for i in range(290):\n",
    "    for k in range(3000):\n",
    "        priont(what am I doing now)\n",
    "for k in range(200):\n",
    "    for k in range(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(x_diff1[1:])\n",
    "test_stationarity(x_diff[2:])\n",
    "test_stationarity(x_diff[:3])\n",
    "for i in range(2000):\n",
    "    for j in range(2300):\n",
    "        fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(x_diff1[1:], lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_pacf(x_diff1[1:], lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try diff 7\n",
    "x_diff7 = full_quantity_total_product['daily_total_product'].diff(7).dropna()\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.plot(x_diff7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(x_diff7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(x_diff7, lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_pacf(x_diff7, lags = 90);\n",
    "smt.graphics.plot_acf(x_diff7, lags = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_diff1_diff7\n",
    "x_diff1_diff7 = full_quantity_total_product['daily_total_product'].diff(1).diff(7).dropna()\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.plot(x_diff1_diff7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(x_diff1_diff7, lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_pacf(x_diff1_diff7, lags = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.tsa.seasonal_decompose(x = full_quantity_total_product['daily_total_product'], freq = 30, model = 'additive', extrapolate_trend=30)\n",
    "res.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(res.trend.diff(1).diff(1).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(res.trend.dropna(), lags = 40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(res.trend.dropna().diff(1).dropna(), lags = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_pacf(res.trend.dropna().diff(1).dropna(), lags = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = sm.tsa.seasonal_decompose(res.trend.dropna(), freq = 365, model = 'additive')\n",
    "res1.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(res1.trend.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res1.trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between x and lags of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = full_quantity_total_product['daily_total_product']\n",
    "plt.scatter(x.shift(1), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.shift(7), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.shift(30), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(test_number, data):\n",
    "    train_number = data.shape[0] - test_number\n",
    "    train_series = data.iloc[:train_number]\n",
    "    test_series = data.iloc[train_number:]\n",
    "    return train_series, test_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test AR(1)\n",
    "x = full_quantity_total_product['daily_total_product'].values\n",
    "x_lag1 = full_quantity_total_product['daily_total_product'].values[1:]\n",
    "plt.scatter(x[:-1], x_lag1)\n",
    "x_arr = np.array(x[:-1]).reshape(-1, 1)\n",
    "y_arr = np.array(x_lag1).reshape(-1, 1)\n",
    "linear_model = LinearRegression().fit(x_arr, y_arr)\n",
    "print('Score: ',linear_model.score(x_arr, y_arr))\n",
    "print('Coef:', linear_model.coef_)\n",
    "plt.plot(x_arr, linear_model.predict(x_arr), c = 'r')\n",
    "plt.xlabel('lag1')\n",
    "plt.ylabel('quantity')\n",
    "plt.title('Scatter plot of quantity vs lag1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "ar_model = AR(x).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(x, (1, 0, 0)).fit(method = 'mle', trend = 'c')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = linear_model.predict(x_arr).ravel()\n",
    "residual = predicted_value - x_arr.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.scatter(predicted_value, residual, c = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All method definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_arima(train_series, p, d, q):\n",
    "    try:\n",
    "        model = ARIMA(train_series, (p, d, q), freq = 'D')\n",
    "        res = model.fit(method = 'mle', trend = 'nc')\n",
    "        aic_value = res.aic\n",
    "    except:\n",
    "        return None, None\n",
    "    return aic_value, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_arima_order(train_series):\n",
    "    rng = range(4)\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_mdl = None\n",
    "    for p in rng:\n",
    "        for d in rng:\n",
    "            for q in rng:\n",
    "                aic_value, mdl = find_summary_arima(train_series, p, d, q)\n",
    "                if (aic_value != None):\n",
    "                    if (aic_value < best_aic):\n",
    "                        best_aic = aic_value\n",
    "                        best_order = (p, d, q)\n",
    "                        best_mdl = mdl\n",
    "    return best_order, best_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day_series, test_day_series = split_train_test(30,full_quantity_total_product_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_parts = sm.tsa.seasonal_decompose(train_series[target_col], freq = 7, model = 'additive')\n",
    "decompose_parts.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(decompose_parts.resid.dropna()['daily_total_product'].values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = decompose_parts.resid.dropna()['daily_total_product'].values\n",
    "np.quantile(x, 0.25), np.quantile(x, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trend\n",
    "remove_seasonal = decompose_parts.observed - decompose_parts.seasonal\n",
    "plt.plot(remove_seasonal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA for train\n",
    "#see the acf_plot\n",
    "best_order, best_mdl = find_best_arima_order(remove_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's predict\n",
    "n_train = train_series.shape[0]\n",
    "n_test = test_series.shape[0]\n",
    "trend_residual_predict, t1, t2 = best_mdl.forecast(n_test)\n",
    "trend_residual_predict = pd.Series(trend_residual_predict, index = test_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_test = [decompose_parts.seasonal.values[(i + n_train)%7][0] for i in range(n_test)]\n",
    "predict_test = trend_residual_predict + seasonal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_seasonal(train_series, test_series, predictor_cols, target_col):\n",
    "    decompose_parts = sm.tsa.seasonal_decompose(train_series[target_col], freq = 7, model = 'additive')\n",
    "    remove_seasonal = decompose_parts.observed - decompose_parts.seasonal\n",
    "    best_order, best_mdl = find_best_arima_order(remove_seasonal)\n",
    "    n_train = train_series.shape[0]\n",
    "    n_test = test_series.shape[0]\n",
    "    trend_residual_predict, t1, t2 = best_mdl.forecast(n_test)\n",
    "    trend_residual_predict = pd.Series(trend_residual_predict, index = test_series.index)\n",
    "    seasonal_for_test = [decompose_parts.seasonal.values[(i + n_train)%7][0] for i in range(n_test)]\n",
    "    predict_for_test = trend_residual_predict + seasonal_for_test\n",
    "    return predict_for_test, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_seasonal_month(train_series, test_series, predictor_cols, target_col):\n",
    "    decompose_parts = sm.tsa.seasonal_decompose(train_series[target_col], freq = 30, model = 'additive')\n",
    "    remove_seasonal = decompose_parts.observed - decompose_parts.seasonal\n",
    "    best_order, best_mdl = find_best_arima_order(remove_seasonal)\n",
    "    n_train = train_series.shape[0]\n",
    "    n_test = test_series.shape[0]\n",
    "    trend_residual_predict, t1, t2 = best_mdl.forecast(n_test)\n",
    "    trend_residual_predict = pd.Series(trend_residual_predict, index = test_series.index)\n",
    "    seasonal_for_test = [decompose_parts.seasonal.values[(i + n_train)%7][0] for i in range(n_test)]\n",
    "    predict_for_test = trend_residual_predict + seasonal_for_test\n",
    "    return predict_for_test, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_method(train_series, test_series, predictor_cols, target_col):\n",
    "    avg = np.mean(train_series[target_col])\n",
    "    preds = np.full(test_series.shape[0], avg)\n",
    "    return preds, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_method(train_series, test_series, predictor_cols, target_col):\n",
    "    last_value = train_series.values[-1]\n",
    "    preds = np.full(train_series.shape[0], last_value)\n",
    "    return preds, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_method(train_series, test_series, predictor_cols, target_col):\n",
    "    X = np.arange(train_series.shape[0]).reshape(-1,1)\n",
    "    y = np.array(train_series[target_col])\n",
    "    x_ = np.arange(train_series.shape[0], train_series.shape[0] + test_series.shape[0]).reshape(-1, 1)\n",
    "    linear_model = LinearRegression().fit(X, y)\n",
    "    preds = linear_model.predict(x_)\n",
    "    return preds, linear_model, linear_model.predict(X) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AR1(train_series, test_series, predictor_cols, target_col):\n",
    "    model = ARIMA(train_series[target_col].values, (1, 0, 0)).fit(method = 'mle', trend = 'c')\n",
    "    print('param:',model.params)\n",
    "    preds,t1,t2 = model.forecast(test_series.shape[0])\n",
    "    return preds, model, model.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_season_method(train_series, test_series, predictor_cols, target_col):\n",
    "    X = train_series[predicted_cols]\n",
    "    X['index'] = np.arange(train_series.shape[0]).reshape(-1,1)\n",
    "    X = np.array(X)\n",
    "    y = np.array(train_series[target_col])\n",
    "    x_ = test_series[predicted_cols]\n",
    "    x_['index'] = np.arange(train_series.shape[0], train_series.shape[0] + test_series.shape[0]).reshape(-1, 1)\n",
    "    x_ = np.array(x_)\n",
    "    linear_model = LinearRegression().fit(X, y)\n",
    "    preds = linear_model.predict(x_)\n",
    "    return preds, linear_model, linear_model.predict(X) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_season_method(train_series, test_series, predictor_cols, target_col):\n",
    "    X = train_series[predicted_cols]\n",
    "    X['index'] = np.arange(train_series.shape[0]).reshape(-1,1)\n",
    "    X = np.array(X)\n",
    "    y = np.array(train_series[target_col])\n",
    "    x_ = test_series[predicted_cols]\n",
    "    x_['index'] = np.arange(train_series.shape[0], train_series.shape[0] + test_series.shape[0]).reshape(-1, 1)\n",
    "    x_ = np.array(x_)\n",
    "    linear_model = LinearRegression().fit(X, y)\n",
    "    resid = linear_model.predict(X) - y\n",
    "    resid_series = pd.Series(resid[:,0], train_series.index)\n",
    "    best_order, best_mdl = find_best_arima_order(resid_series)\n",
    "    seasonal_for_test = linear_model.predict(x_)[:,0]\n",
    "    trend_residual_predict, t1, t2 = best_mdl.forecast(n_test)\n",
    "    trend_residual_predict = pd.Series(trend_residual_predict, index = test_series.index)\n",
    "    predict_for_test = trend_residual_predict + seasonal_for_test\n",
    "    return predict_for_test, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting_process(train_series, test_series, func_model, type_model, predictor_cols, target_col):\n",
    "    total_mse = 0\n",
    "    total_mad = 0\n",
    "    total_mape = 0\n",
    "    n = 0\n",
    "    result_columns = ['period', 'demand', 'forecast', 'error', 'abs_error','MSE', 'MAD', 'percent_error', 'MAPE']\n",
    "    result = pd.DataFrame([], columns = result_columns)\n",
    "    selected_cols = list(set(predictor_cols).union(set(target_col)))\n",
    "    preds, model, resid = func_model(train_series[selected_cols], test_series[selected_cols], predictor_cols, target_col)\n",
    "    for index, row in test_series.iterrows():\n",
    "        pred = preds[n]\n",
    "        error = pred - row[target_col].values[0]\n",
    "        abs_error = math.fabs(error)\n",
    "        percent_error = abs_error/row[target_col].values[0]\n",
    "        n = n + 1\n",
    "        total_mse += error ** 2\n",
    "        total_mad += abs_error\n",
    "        total_mape += percent_error\n",
    "        mse = total_mse/n\n",
    "        mad = total_mad/n\n",
    "        mape = total_mape/n\n",
    "        v = [index, row[target_col].values[0], pred, error, abs_error, mse, mad, percent_error, mape]\n",
    "        result = result.append([dict(zip(result_columns, v))])\n",
    "    result.index = np.arange(test_series.shape[0])\n",
    "    f = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(train_series.index, train_series[target_col].values, label = 'train')\n",
    "    plt.plot(test_series.index,test_series[target_col].values, label = 'test')\n",
    "    plt.plot(test_series.index, result['forecast'], color = 'k', label = 'forecast')\n",
    "    plt.xlabel('day')\n",
    "    plt.ylabel('quantity')\n",
    "    plt.title(type_model)\n",
    "    plt.legend();\n",
    "    return result, model, resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ARIMA Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arimaseasonal_result, model, resid = forecasting_process(train_day_series, test_day_series, arima_seasonal, 'arima_seasonal', [], ['daily_total_product'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test avg method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result, model, resid = forecasting_process(train_day_series, test_day_series, average_method, 'avarage', [], ['daily_total_product'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Naive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_result, model, resid = forecasting_process(train_day_series, test_day_series, naive_method, 'Naive', [], ['daily_total_product'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test drift method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_result, model,resid = forecasting_process(train_day_series, test_day_series, drift_method, 'drift', [], ['daily_total_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resid, alpha = 0.7, label = 'varaiance = {:.3f}'.format(np.std(resid)))\n",
    "plt.hlines(0, xmin = 0, xmax = len(resid), color = 'r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(resid, lags = 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AR(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar1_result, model, resid = forecasting_process(train_day_series, test_day_series, AR1, 'AR1', [], ['daily_total_product'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resid, alpha = 0.7, label = 'varaiance = {:.3f}'.format(np.std(resid)))\n",
    "plt.hlines(0, xmin = 0, xmax = len(resid), color = 'r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(resid, lags = 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Seasonal Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cols = list(set(train_day_series.columns) - set(['daily_total_product', 'week', 'year']))\n",
    "target_col = ['daily_total_product']\n",
    "seasonal_naive_result, model, resid = forecasting_process(train_day_series, test_day_series, naive_season_method, 'naive', predicted_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resid, alpha = 0.7, label = 'varaiance = {:.3f}'.format(np.std(resid)))\n",
    "plt.hlines(0, xmin = 0, xmax = len(resid), color = 'r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(resid, lags = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test upgrade season method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cols = list(set(train_day_series.columns) - set(['daily_total_product', 'week', 'year']))\n",
    "target_col = ['daily_total_product']\n",
    "upgrade_seasonal_result, model, resid = forecasting_process(train_day_series, test_day_series, upgrade_season_method, 'upgrade_season', predicted_cols, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "all_result_name = ['arimaseasonal_result', 'avg', 'naive', 'drift', 'ar1', 'season_naive', 'upgrade_seasonal']\n",
    "all_result_method = [arimaseasonal_result, avg_result, naive_result, drift_result, ar1_result, seasonal_naive_result, upgrade_seasonal_result]\n",
    "for result_name, result_method in zip(all_result_name, all_result_method):\n",
    "    plt.plot(result_method['MAPE'], label = result_name)\n",
    "plt.xlabel('test_size')\n",
    "plt.ylabel('MAPE')\n",
    "plt.title('MAPE for all methods')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(full_quantity_product[['daily_total']]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.tsa.seasonal_decompose(full_quantity_product[['daily_total']], freq = 30, model = 'additive')\n",
    "res.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = sm.tsa.seasonal_decompose(res.trend.dropna(), freq = 356, model = 'additive')\n",
    "res1.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = sm.tsa.seasonal_decompose(res.trend.dropna(), freq = 356, model = 'multiplicative')\n",
    "res1.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_week = sm.tsa.seasonal_decompose(week_data['weekly_total'], freq = 53, model = 'additive')\n",
    "fig = res_week.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(res_week.resid**2)/len(res_week.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.graphics.plot_acf(week_data['weekly_total'], alpha=0.5);\n",
    "smt.graphics.plot_pacf(week_data['weekly_total'], alpha = 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(week_data['weekly_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_data_series = week_data[['weekly_total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_number = 8\n",
    "train_number = week_data_series.shape[0] - test_number\n",
    "train_series = week_data_series.loc[:train_number - 1]\n",
    "test_series = week_data_series.loc[train_number:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_exp_smoothing(train_series, test_series, type_model):\n",
    "    total_mse = 0\n",
    "    total_mad = 0\n",
    "    total_mape = 0\n",
    "    n = 0\n",
    "    result_columns = ['period', 'demand', 'forecast', 'error', 'abs_error','MSE', 'MAD', 'percent_error', 'MAPE']\n",
    "    result = pd.DataFrame([], columns = result_columns)\n",
    "    if type_model == 'SimpleExpSmoothing':\n",
    "        model = SimpleExpSmoothing(train_series.values[:, 0])\n",
    "        fit = model.fit()\n",
    "    elif type_model == 'Holt':\n",
    "        model = Holt(train_series.values[:, 0])\n",
    "        fit = model.fit(smoothing_level = 0.3, smoothing_slope = 0.05)\n",
    "    preds = fit.forecast(test_series.shape[0])\n",
    "    for index, row in test_series.iterrows():\n",
    "        pred = preds[n]\n",
    "        error = pred - row.values[0]\n",
    "        abs_error = math.fabs(error)\n",
    "        percent_error = abs_error/row.values[0]\n",
    "        n = n + 1\n",
    "        total_mse += error ** 2\n",
    "        total_mad += abs_error\n",
    "        total_mape += percent_error\n",
    "        mse = total_mse/n\n",
    "        mad = total_mad/n\n",
    "        mape = total_mape/n\n",
    "        v = [index, row.values[0], pred, error, abs_error, mse, mad, percent_error, mape]\n",
    "        result = result.append([dict(zip(result_columns, v))])\n",
    "        train_series = train_series.append(pd.DataFrame([(row.values[0],)], index = [index], columns = train_series.columns))\n",
    "    f = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(train_series.index, train_series.values[:, 0])\n",
    "    plt.plot(test_series.index,test_series.values[:, 0])\n",
    "    plt.plot(test_series.index, result['forecast'], color = 'k')\n",
    "    plt.title(type_model)\n",
    "    plt.legend();\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = simple_exp_smoothing(train_series, test_series, 'SimpleExpSmoothing')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = simple_exp_smoothing(train_series, test_series, 'Holt')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_arima(train_series, p, d, q):\n",
    "    try:\n",
    "        model = ARIMA(train_series, (p, d, q))\n",
    "        res = model.fit(method = 'mle', trend = 'nc')\n",
    "        aic_value = res.aic\n",
    "    except:\n",
    "        return None, None\n",
    "    return aic_value, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_arima_order(train_series):\n",
    "    rng = range(4)\n",
    "    best_aic = np.inf\n",
    "    best_order = None\n",
    "    best_mdl = None\n",
    "    for p in rng:\n",
    "        for d in rng:\n",
    "            for q in rng:\n",
    "                aic_value, mdl = find_summary_arima(train_series, p, d, q)\n",
    "                if (aic_value != None):\n",
    "                    if (aic_value < best_aic):\n",
    "                        best_aic = aic_value\n",
    "                        best_order = (p, d, q)\n",
    "                        best_mdl = mdl\n",
    "    return best_order, best_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_process(train_series, test_series):\n",
    "    total_mse = 0\n",
    "    total_mad = 0\n",
    "    total_mape = 0\n",
    "    n = 0\n",
    "    result_columns = ['period', 'demand', 'forecast', 'error', 'abs_error','MSE', 'MAD', 'percent_error', 'MAPE', 'best_order']\n",
    "    result = pd.DataFrame([], columns = result_columns)\n",
    "    best_order, best_mdl = find_best_arima_order(train_series)\n",
    "    preds, t1, t2= best_mdl.forecast(test_series.shape[0])\n",
    "    for index, row in test_series.iterrows():\n",
    "        pred = preds[n]\n",
    "        error = pred - row.values[0]\n",
    "        abs_error = math.fabs(error)\n",
    "        percent_error = abs_error/row.values[0]\n",
    "        n = n + 1\n",
    "        total_mse += error ** 2\n",
    "        total_mad += abs_error\n",
    "        total_mape += percent_error\n",
    "        mse = total_mse/n\n",
    "        mad = total_mad/n\n",
    "        mape = total_mape/n\n",
    "        v = [index, row.values[0], pred, error, abs_error, mse, mad, percent_error, mape, best_order]\n",
    "        result = result.append([dict(zip(result_columns, v))])\n",
    "        train_series = train_series.append(pd.DataFrame([(row.values[0],)], index = [index], columns = train_series.columns))\n",
    "    plt.figure(figsize = (15, 6))\n",
    "    plt.plot(train_series.index, train_series.values[:, 0])\n",
    "    plt.plot(test_series.index,test_series.values[:, 0])\n",
    "    plt.plot(test_series.index, result['forecast'], color = 'k')\n",
    "    plt.title('ARIMA')\n",
    "    plt.legend();\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = arima_process(train_series, test_series)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonalExponentialSmoothing(train_series, test_series):\n",
    "    total_mse = 0\n",
    "    total_mad = 0\n",
    "    total_mape = 0\n",
    "    n = 0\n",
    "    result_columns = ['period', 'demand', 'forecast', 'error', 'abs_error','MSE', 'MAD', 'percent_error', 'MAPE']\n",
    "    result = pd.DataFrame([], columns = result_columns)\n",
    "    result = pd.DataFrame([], columns = result_columns)\n",
    "    model = ExponentialSmoothing(train_series.values[:, 0], trend = 'add', seasonal = 'add', seasonal_periods = 53)\n",
    "    fit = model.fit()\n",
    "    preds = fit.forecast(test_series.shape[0])\n",
    "    for index, row in test_series.iterrows():\n",
    "        pred = preds[n]\n",
    "        error = pred - row.values[0]\n",
    "        abs_error = math.fabs(error)\n",
    "        percent_error = abs_error/row.values[0]\n",
    "        n = n + 1\n",
    "        total_mse += error ** 2\n",
    "        total_mad += abs_error\n",
    "        total_mape += percent_error\n",
    "        mse = total_mse/n\n",
    "        mad = total_mad/n\n",
    "        mape = total_mape/n\n",
    "        v = [index, row.values[0], pred, error, abs_error, mse, mad, percent_error, mape]\n",
    "        result = result.append([dict(zip(result_columns, v))])\n",
    "        train_series = train_series.append(pd.DataFrame([(row.values[0],)], index = [index], columns = train_series.columns))\n",
    "    f = plt.figure(figsize = (15, 6))\n",
    "    plt.plot(train_series.index, train_series.values[:, 0])\n",
    "    plt.plot(test_series.index,test_series.values[:, 0])\n",
    "    plt.plot(test_series.index, result['forecast'], color = 'k')\n",
    "    plt.title('Seasonal exponential smoothing')\n",
    "    plt.legend();\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonalExponentialSmoothing(train_series, test_series)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series.iloc[0].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleBootstrap(len_block, train_series, test_series):\n",
    "    #moving block bootstrap\n",
    "    #create data points\n",
    "    size_train = train_series.shape[0]\n",
    "    n_points = train_series.shape[0] - len_block + 1\n",
    "    X = np.zeros((n_points, len_block - 1))\n",
    "    y = np.zeros(n_points)\n",
    "    for i in range(n_points):\n",
    "        for j in range(len_block - 1):\n",
    "            X[i,j] = train_series.iloc[i + j].values[0]\n",
    "        y[i] = train_series.iloc[i + len_block - 1].values[0]\n",
    "    liner_model = LinearRegression().fit(X, y)\n",
    "    total_mse = 0\n",
    "    total_mad = 0\n",
    "    total_mape = 0\n",
    "    n = 0\n",
    "    result_columns = ['period', 'demand', 'forecast', 'error', 'abs_error','MSE', 'MAD', 'percent_error', 'MAPE']\n",
    "    result = pd.DataFrame([], columns = result_columns)\n",
    "    values = np.zeros(len_block - 1)\n",
    "    for i in range(len_block - 1):\n",
    "        values[i] = train_series.iloc[train_series - len_block + i + 2]\n",
    "    for index, row in test_series.iterrows():\n",
    "        pred = liner_model.predict(values)\n",
    "        error = pred - row.values[0]\n",
    "        abs_error = math.fabs(error)\n",
    "        percent_error = abs_error/row.values[0]\n",
    "        n = n + 1\n",
    "        total_mse += error ** 2\n",
    "        total_mad += abs_error\n",
    "        total_mape += percent_error\n",
    "        mse = total_mse/n\n",
    "        mad = total_mad/n\n",
    "        mape = total_mape/n\n",
    "        v = [index, row.values[0], pred, error, abs_error, mse, mad, percent_error, mape]\n",
    "        result = result.append([dict(zip(result_columns, v))])\n",
    "        train_series = train_series.append(pd.DataFrame([(row.values[0],)], index = [index], columns = train_series.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
